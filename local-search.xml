<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>三天入门NLP-2023华中杯复盘</title>
    <link href="/2023/05/27/2023%E5%8D%8E%E4%B8%AD%E6%9D%AF%E5%A4%8D%E7%9B%98/"/>
    <url>/2023/05/27/2023%E5%8D%8E%E4%B8%AD%E6%9D%AF%E5%A4%8D%E7%9B%98/</url>
    
    <content type="html"><![CDATA[<h1 id="2023华中杯复盘"><a href="#2023华中杯复盘" class="headerlink" title="2023华中杯复盘"></a>2023华中杯复盘</h1><p>爆肝的五一，选了B题，听说估计只有20%的人选了B，有点不知道天高地厚hhh，毕竟<del>三天入门NLP</del>。</p><p>很宝贵的经历，毕竟是大学生涯中第一次参加数模比赛，写个复盘留个记录。</p><h2 id="前期"><a href="#前期" class="headerlink" title="前期"></a>前期</h2><p>大概从3月初准备参赛，前后断断续续准备了两个月。</p><p>学习路线大概是这样的：</p><ul><li><p>建模：</p><p>一开始准备follow司守奎老师的黄皮书，后来发现现有的对于概率论、统计的知识储备有点不够，难以坚持读下去，看完了线性规划，用Matlab敲了点代码，遂摆烂。</p><p>后来在一个师兄建议下去看了看清风的课程，虽然好像评价普遍不是很高（？），但是从这次参赛的经验来看，作为大一学生的入门还是完全够用的。（毕竟我也没花钱（逃~，放个油管的链接<a href="https://www.youtube.com/watch?v=_gRh9u4LuZs&list=PLvce_oy4ggsHzrmgBz8vwQqRmezDOzo1N">清风数学建模全套视频</a>），然后断断续续跟着视频学了一些但不多：$TOPSIS$、层次分析、插值算法、分类问题、回归分析、聚类、拟合、图论最短路径、相关系数。</p><p>有几章学的挺仔细，有些因为知识储备的原因学的很浅。</p><p>po下自己4、5月写的笔记<a href="https://github.com/Allinnow/Mathematical_Modeling">Mathematical_Modeling-笔记</a>，也有一些是在知乎、wiki、csdn、掘金等各各乱七八糟的平台看的文章写的笔记(其实我感觉个人写的博客反而对各种东西的讲解很好，清晰、易懂),不过我的笔记可能只有自己看得懂hhh。</p></li><li><p>Coding:</p><p>这个准备的不多，只学了学Matlab的一些常规操作（伏笔，哭死）</p></li><li><p>写作：<br>followB站<a href="https://www.bilibili.com/video/BV1Ci4y1c7Ld/?spm_id_from=..search-card.all.click&vd_source=132d4ba23d3848c8fb46c04c09807f90">清风的公开课</a>，没看完。</p></li></ul><p>四五月花的时间，不算少但是也没挤压所有时间来准备，5月过半后就基本没怎么太学建模的东西了，<del>基本摆烂了</del>。总体来说准备不是特别充分，主要还是囿于概统、机器学习、Python这些东西的知识欠缺。</p><h2 id="痛苦的三天"><a href="#痛苦的三天" class="headerlink" title="痛苦的三天"></a>痛苦的三天</h2><h3 id="拿到题目"><a href="#拿到题目" class="headerlink" title="拿到题目"></a>拿到题目</h3><p>看了看ABC，A应该是挺难的，C应该还算好做（后来证实了这个想法，md但是当时觉得C的题目难读），觉得反正都是得现场学，不如选个题目意思清晰的题，B题也还算挺有意思的。</p><p>然后就选了B题，让我接下来三天都挺后悔的决定。</p><p>上午在知网上看了一圈paper，其实这类度量估计题目难度、相似度的工作早有人做过。</p><p>具体可参考：</p><ol><li>张睿，基于 k-means 的中文文本聚类算法的研究与实现</li><li>周萍，基于语义分析的文本相似性度量研究及应用</li><li>范缜，文本聚类技术综述</li><li>吴启纲，中文文本聚类算法的研究与实现</li><li>黄承慧,一种结合词项语义信息和$TF-IDF$方法的文本相似度量方法</li><li>熊蜀光，题目相似度的计算方法及装置</li></ol><p>最开始是我们想到的大致类似于第6篇论文的思路，非常简单暴力的想法，把所有词放在一起，单纯用词汇的相似性来计算刻画相似性，而且仔细一想这样是有大问题的。这个地方也引出了我对B题的最大的想法：</p><p><strong>这个题考察的根本不是NLP。</strong></p><p>实际上，拿到题目的晚上我就一直在想，要得到每个题的特征数据，如果不是手动做的话，可能要基于各种深度学习、语料训练做非常复杂的工作，由于是第一次参加数模，我当时并没有意识到想用NLP技术得到每个题目作为向量的各个特征值是多么困难的一件事。</p><p>否认了这个想法后，整个做题的思路实际上比较清晰了。</p><p>首先是VSM,上述第6篇论文提出了就是一个基础的<strong>向量空间模型（Vector  Space  Model,即VSM）</strong>这也是本题开始进行处理的基础建模手段，没什么好说的。</p><p>然后就来到了我认为的本题的核心：</p><p>把每个题目的关键信息提取出来，包括：题目中的运算法则，数据类型，数据范围、数据量、解题步骤、解题方法。</p><p>因此我觉得<strong>能找到多少较好的度量指标、如何量化</strong>，可能算是本题的着重考察点，而这并不牵涉到多么高深的NLP知识。</p><p>而我们论文的主要工作或者说和那些已有Paper基础上有所不同的就只是用这些度量指标代替了原有词汇(比如”小红”“老师”这样大量的无意义无信息的词汇)。遗憾的是我并没有想到很好的量化方法，只是采取了一种比较无脑的想法——把每个指标记为中文文本，like “1”“2”“解方程”，从最后聚类的结果看，应该不是很好的方式，暂时不知道最好的解决方案是什么。</p><p><img src="https://dyh123.oss-cn-beijing.aliyuncs.com/image/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-05-25%20191056.png" alt="构建词表"></p><p>后面再用了些TF-IDF，欧氏距离度量，用spss做了系统聚类，这都算是比较自然而然的想法。</p><p>值得一提的是，现在我还是不太清楚评估题目难度的最优方案是什么，熵权？多元回归分析？我们队用的是多元回归分析，算是无奈之举，多元回归分析有一个致命的缺点，在我看来这也是我们论文的最大问题。</p><p><img src="https://dyh123.oss-cn-beijing.aliyuncs.com/image/image-20230525191442322.png" alt="多元回归系数的确定"></p><p>这一步操作肯定是有问题的，作为非NLP专业的学生，回归系数根本算不出来。这也是本次比赛我最大的疑惑，这一问是否需要使用较为高级的NLP技术。</p><h2 id="写在后面"><a href="#写在后面" class="headerlink" title="写在后面"></a>写在后面</h2><p>在本次参赛过程中，实际上我参与了建模、代码、数据处理、论文写作的过程，我的感受是:<strong>要么全能，要么就是混</strong>，不是很理解为什么传统上有“论文手”这样一个分工，建模了直接写论文效率绝对会高一个量级（沉痛教训）。之前在知乎看到的一个分工，我觉得值得一试：两个建模手同时建模，写一点代码，直接把过程写上论文，再一个编程手all in比较困难的代码就好。</p><p>还有感受特别深的就是:数模还是越早参加越好，不是所有的原料准备好了再下锅，数模竞赛的准备绝对不是线性的，而是爆发式的。</p><p>再就是自己需要提升的地方：</p><ul><li>爆发式的学习和长时间沉浸式的专注很重要</li><li>多搜集文献，多学习前人的idea</li></ul><p>另外感受特别深的一点是，在LLM爆发的时代，以后的数学建模比赛又该何去何从呢，ChatGPT在数模比赛中作为辅助工作流的存在，有多好用懂得都懂，一位师兄说：</p><blockquote><p>重要的是你想实现什么，不是怎样实现。</p></blockquote><p>很受用，深有感触，能想出好的idea才是人之为人的独特之处，从这个意义上讲，建模的学习、积累才是重中之重。</p><p>2023年5月25日晚</p><p>财大文瀚</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学建模</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
